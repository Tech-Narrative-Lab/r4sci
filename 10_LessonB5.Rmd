---
  title: B5.  Control Language
---

```{r Environment set-up, include= FALSE}
remove(list= objects())
```

```{r Document-Wide Knitr Options, include= FALSE }
knitr::opts_chunk$set(fig.width=6, fig.height=6) 
knitr::opts_chunk$set(comment= "")
knitr::opts_chunk$set(prompt= TRUE)
```

[Go to the Table of Contents](00_TableOfContents.html)

[Return to Last Lesson (B4: Visualization)](09_LessonB4.html)

This lesson will introduce you to tools and techniques for writing more sophisticated programs. 

### Evaluating Your Code

Much like writing a book, good code typically requires multiple drafts.  For the first draft, you might write the section headings first as an outline and then develop code for each section that accomplishes the specified task.  For the second draft, you might adjust your code based on what you learned from the first draft.  For example, changing code in early sections to better support the code in later sections or changing variables to more accurately reflect how they were used.  For the third draft, you might rewrite sections of code that take a long time to run or have a large RAM footprint.  For the fourth draft (if needed), you might switch from testing the code against a small test dataset to testing it against data at full scale and repeat the time/resource footprint checks of the third draft.

Below is a simple example of how to time test your code.  The example tests two methods of executing a function - providing a vector of arguments and executing the function separately for each element of the vector.  Since R is an interpreted language, vectorized functions in R run more quickly than the same calculations done as serial expressions because the interpreter has to process each line separately.

```{R Vectorized Vs Serial}

## Define a nonsense function for testing purposes
NonSense <- function(x){
	x <- abs(x)
	y <- choose(x^2 , x)
	x <- x * y * pi
	x <- sqrt(x)
	x <- diff(x)
	x <- median(x)
	return(x)
}


## Generate random integers
random_numbers <- round(runif(min= 1, max= 10^3, n= 2^4))
time_elapsed <- as.list( rep(NA, 2) )
names(time_elapsed) <- c("Vectorized", "Serial")

## Execute NonSense() in vectorized form
temp_file <- rep(NA, 2^4)
time_elapsed$Vectorized <- Sys.time()
temp_file <- NonSense(random_numbers)
time_elapsed$Vectorized <- Sys.time() - time_elapsed$Vectorized

## Execute NonSense() element by element ("Serialized")
temp_file <- rep(NA, 2^4)
time_elapsed$Serial <- Sys.time()
temp_file[1]  <- NonSense(random_numbers[1])
temp_file[2]  <- NonSense(random_numbers[2])
temp_file[3]  <- NonSense(random_numbers[3])
temp_file[4]  <- NonSense(random_numbers[4])
temp_file[5]  <- NonSense(random_numbers[5])
temp_file[6]  <- NonSense(random_numbers[6])
temp_file[7]  <- NonSense(random_numbers[7])
temp_file[8]  <- NonSense(random_numbers[8])
temp_file[9]  <- NonSense(random_numbers[9])
temp_file[10] <- NonSense(random_numbers[10])
temp_file[11] <- NonSense(random_numbers[11])
temp_file[12] <- NonSense(random_numbers[12])
temp_file[13] <- NonSense(random_numbers[13])
temp_file[14] <- NonSense(random_numbers[14])
temp_file[15] <- NonSense(random_numbers[15])
temp_file[16] <- NonSense(random_numbers[16])
time_elapsed$Serial <- Sys.time() - time_elapsed$Serial

## display time elapsed
lapply( time_elapsed, round, digits= 3)

## delete temporary variables
remove(time_elapsed, temp_file, random_numbers, NonSense)

```

In addition to measuring run time for a function, we can also measure run time for all functions within a block of code using `Rprof()` and `summaryRprof()`.  `Rprof()` takes snapshots of R as it executes the code and records which functions were in use during each snapshot.  It writes the sampling data to a text file that is called `Rprof.out` by default.  `summaryRprof()` reads that text file and tabulates how often each function appears.  The percentages sum to greater than one because functions are nested in each other.  For example, in the 20th sample, `var()` was running inside `sd()`, which was running inside `rnorm()`, which as running inside `AddNoise()`.  This corresponds to the second to last line of code in the `AddNoise()` function().  Based on these outputs, a programmer might reasonably conclude that random number generation is responsible for the majority of the computational cost of running this code.

```{R Rprof()}

## make function that adds noise to a dependent variable in
  ## order to create an imperfect predictor for linear modeling
AddNoise <- function(x) {
	## give x an arbitrary scale
	x <- {x - mean(x)} / sd(x)
	scale_numbers <- runif(min= -100, max= 1000, n= 2)
	x <- {x * scale_numbers[1]} + scale_numbers[2]
	
	## add data noise to x and express the results
	x <- x + rnorm(n= length(x), sd= sd(x) / sqrt(length(x)))
	return(x)
}

## initialize R profiler to see which functions are responsible
  ## for extending the run time.
Rprof()

## create a dataset with a predictor and independent variable
the_dataset <- data.frame(y= rpois(n= 10^7, lambda= 5))
the_dataset$x <- AddNoise(the_dataset$y)

## express summary of results from Rprof.
Rprof(NULL)
# summaryRprof()$by.total  ## This expression would print:

#              total.time total.pct self.time self.pct
# "AddNoise"         0.50     65.79      0.00     0.00
# "rnorm"            0.40     52.63      0.36    47.37
# "rpois"            0.26     34.21      0.26    34.21
# "data.frame"       0.26     34.21      0.00     0.00
# "var"              0.08     10.53      0.08    10.53
# "sd"               0.08     10.53      0.00     0.00
# "-"                0.02      2.63      0.02     2.63
# "/"                0.02      2.63      0.02     2.63
# "+"                0.02      2.63      0.02     2.63


## express a few lines from the underlying Rprof.out file to show how the raw
  ## data is formatted

# data.frame(
# 	"SampleNumber"= 38:40,
# 	"FunctionsInUse"= readLines("Rprof.out")[38:40]
# 	) ## This expression would print:

#   SampleNumber                 FunctionsInUse
# 1           18                "/" "AddNoise" 
# 2           19 "var" "sd" "rnorm" "AddNoise" 
# 3           20 "var" "sd" "rnorm" "AddNoise" 

## delete demonstration objects and files
remove(the_dataset, AddNoise)
file.remove("Rprof.out")


```

As we discuss other programming techniques in this chapter, we will apply these time performance measurement strategies in order to understand the pros and cons of programming techniques.

### Loops and Conditionals

#### Loops

Vectorized functions and apply family function are the most efficient methods to execute an operation in bulk in R.  However, there are circumstances in which the next execution of an operations depends on the outcome of the previous execution.  For example, a Fibonacci sequence is a set of integers in which each integer after the first two is the sum of the previous two digits.  _The_ Fibonacci sequence typically starts from 0, 1, in which case the sequence would proceed as:

```
Start: 0, 1
0 + 1 = 1
1 + 1 = 2
1 + 2 = 3
2 + 3 = 5
3 + 5 = 8
5 + 8 = 13
```

And so forth.  Fibonacci sequences cannot be generated using vectorized operations or apply family functions because the operation of generating the next digit depends on the result from the previous two attempts to generate digits.  Loops are the standard computational tool for conducting such operations.

R has three looping functions, `repeat`, `while(){}`, and `for(){}`.  It also has tools for deciding what to do during each iteration, including `if(){}` and `else{}`.  This example also uses `print()`, which outputs messages to console in a manner that is cleaner than object expression.

```{R Loops - Fibonacci Demo}

## make a simple Fibonacci sequence generator
Fibonacci <- function(x) {
	y <- sum( x[length(x) - 0:1])
	return(c(x, y))
}

## repeat will continue to repeat an operations forever, unless you provide a
  ## break statement. Note the lack of parentheses after repeat.  The looping
  ## and conditional control words operate differently than the standard R
  ## function syntax.
fib_sequence <- 0:1
repeat {
	fib_sequence <- Fibonacci(fib_sequence) ## generate the next number
	if (max(fib_sequence) > 10) break ## end the loop if next number is > 10
}

fib_sequence

## while() incorporates the stopping condition as an argument
fib_sequence <- 0:1
while (max(fib_sequence) <= 10) {
	fib_sequence <- Fibonacci(fib_sequence) ## generate the next number
}

fib_sequence

## for() iterates through a defined set of values which can serve as useful
  ## indices for bracket operations or do other useful work.  
fib_sequence <- 0:1

for (iteration_value in 1:6) {
	fib_sequence <- Fibonacci(fib_sequence)
	
	print(paste( 'iteration_value=', iteration_value ))
	print(paste( 'New digit=', fib_sequence[iteration_value + 2] ))
}

fib_sequence

## delete demonstration objects
remove(fib_sequence, iteration_value, Fibonacci)


```

The key strength of a loop is its ability to make decisions each round.  This is computationally expensive because the R interpreter must reengage each round, but can enable your program to do sophisticated decision-making.  In the example below, the loop checks the clock once per second and announces the time if the current clock second is divisible by 5.  This example also uses `Sys.sleep()`, which is an R function that causes execution of an R script to pause for a specified number of seconds.

```{R Ticking Clock}

the_time <- Sys.time()

repeat {
	
	## force R to wait for one second on each iteration of the loop, so that the
	  ## loop does not iterate too quickly
	Sys.sleep(1) 
	
	## record the current time
	the_time <- c(the_time, Sys.time()) 
	
	## determine if the current clock second is divisible by 5
	anouncement_decision <- substring(max(the_time), 18, 19)
	anouncement_decision <- as.numeric(anouncement_decision)
	
	## if the current clock second is not divisible by five, print "tick".  If it
	  ## is divisible by 5, print the current time.
	if (anouncement_decision %% 5 != 0) {
		print("tick")
	} else {
		print(paste("The current time is", Sys.time()))
			}

	if (max(the_time) - min(the_time) > 9) break ## end loop after 9 seconds
	
}

## delete demonstration objects
remove(the_time, anouncement_decision)

```

Loops and apply family functions can do many of the same tasks.  However, apply family functions only engage the R interpreter once, which often makes them much more efficient.  Strive to only use loops under three conditions:

1. No apply family function can do the task because the outputs from the last round of calculation serve as inputs for the next.

2. While an apply family function can do the task, it would take too much RAM to process all elements at once so the calculation needs to be done one element at a time.

3. You did side-by-side time trials of your loop and an equivalent apply family function and the loop was consistently faster.  Starting with R v4.0, this is becoming less rare -- v4.0 significantly improved loop efficiency.

To illustrate the nuances of this, here is a side by side performance comparison of `for(){}` and `apply()` on an averaging task.  In this example, `for(){}` has approximately 120-140% of the run time of `apply()`.  However, if you replace the looped line with the one below it (commented out in the example), the performance difference between the two decreases significantly.  In a portion of trials, the loop is actually faster. This change suggests that memory allocation is putting the loop at a disadvantage -- each iteration of R is having to reallocate memory for loop_object because the size of one of its elements has changed and this is dragging down the loop's performance.

```{R for() v lapply}

## CONDUCT A TIME TRIAL ====================

## Note: typically, one would test each expression multiple times and then
  ## average the results.  This example does a single trial each to make the
  ## code easier to follower.

## make a large data object
loop_object <- lapply_object <- mapply(
	FUN= seq,
	from= 1,
	to= 2^{1:20},
	by= pi * 0.1
	)
time_trial <- rep(NA, 2)
names(time_trial) <- c("for", "lapply")

## use a for(){} loop to calculate means

alternative_loop_result <- rep(NA, length(loop_object))

start_time <- Sys.time()
for(iter in seq(from= 1, along= loop_object)){
	loop_object[[iter]] <- mean(loop_object[[iter]]) ## comment line to improve loop performance
	## alternative_loop_result[iter] <- mean(loop_object[[iter]]) ## uncomment line to improve loop performance
	}
time_trial['for'] <- Sys.time() - start_time

## use lapply() to do the same
start_time <- Sys.time()
lapply_object <- lapply(lapply_object, mean)
time_trial['lapply'] <- Sys.time() - start_time

## compare the results (the exact performance will vary from trial to trial
  ## as well as varying according to your hardware.  For best results, repeat
  ## your time trial multiple times and then average the results.
round(time_trial / min(time_trial), 2)

## remove demonstration objects
remove(time_trial, start_time, lapply_object, loop_object, iter,
	alternative_loop_result)

```

#### Optimization

Optimization problems form an important subset of "last-result-informs-next" loops.  In an optimization loop, one tries something, observes the results, and then tries something different to get better results.  In R, the `optim()` function does this task.  `optim()` has two key arguments. `fn=` is a function for which you want to find the optimal arguments, formulated so that it will return a "score" and lower scores indicate better results.  `par=` is an initial set of arguments to supply the function. `optim()` starts with those values and tries variations on them until it finds values that produce the lowest score.

To give an example, imagine that you have a geometric point at coordinate (10, 10) and you want to find another point that is exactly 2 units away.  Optimization could be used to find solutions to this problem, as demonstrated below.

```{R optim()}
ScoreFunction <- function(x) {
	y <- x[2]
	x <- x[1]
  the_distance <- {x - 10}^2 + {y - 10}^2
  the_distance <- abs(2 - sqrt(the_distance))
  return(the_distance)
}


## find a solution to this problem, using (0,0) as the initial parameters
optim(fn= ScoreFunction, par= c(0, 0))

## -- plot how the solutions vary, depending on the initial parameters

## define 441 distinct starting positions
initial_parameters <- data.frame(
	row_number= 1:21^2,
	x_initial= rep(0:20, each= 21),
	y_initial= rep(0:20, times= 21),
	x_solution= NA,
	y_solution= NA
	)

## in order to provide further demonstrations of loops in action, use a loop
  ## to test which solution optim() finds, given different starting positions.
  ## A lapply() or mapply() would likely be more efficient.
for(iter in initial_parameters$row_number) {
	
	initial_parameters[iter, c("x_solution", "y_solution")] <- optim(
		fn= ScoreFunction,
		par= initial_parameters[iter, c("x_initial", "y_initial")],
		control= list(maxit= 10^4)
			)$par
	
}

## color points according to circle quadrant
initial_parameters$point_colors <- ifelse(initial_parameters$x_initial > 10, 10,
	0) + ifelse(initial_parameters$y_initial > 10, 1, 0)
initial_parameters$point_colors <- as.numeric(as.factor(
	initial_parameters$point_colors))
initial_parameters$point_colors <- rainbow(n= 4, s= 0.7, v= 0.7)[
	initial_parameters$point_colors]

## plot all solutions found
plot.new()
par(xpd= TRUE)
plot.window(
	xlim= range(initial_parameters$x_solution),
	ylim= range(initial_parameters$x_solution),
	asp= 1
	)
abline(h= 0:20, v= 0:20, col= "gray70")
points(
	x= initial_parameters$x_solution,
	y= initial_parameters$y_solution,
	pch= 16,
	cex= 0.5,
	col= initial_parameters$point_colors
	)
text(x= 7.8, y= 12:8, labels= 12:8, font= 2)
text(y= 7.8, x= 12:8, labels= 12:8, font= 2)

## delete demonstration objects
remove(initial_parameters, iter, ScoreFunction)

```

In the example above, there are many solutions to this problem, since any point in the ring around (10, 10) qualifies.  `optim()` will find one of those solutions, not all.  While the same initial parameters will always lead to the same solution, similar initial parameters do not necessarily lead to similar solutions. This is why the point colors on the chart (which roughly categorize the initial parameters into four groups) are randomly distributed around the circle.

#### Conditionals

Loops present many good opportunities to use conditionals like `if(){}` and `else{}` but conditionals can be useful outside of loops as well.  For example, conditionals can enable you to toggle into a test mode while you are modifying/debugging code.  This example also uses `warning()` wish is the R function for generating warning messages.

```{R Conditionals for testing}


## FOR THIS EXAMPLE, IMAGINE YOU READ IN A 10m ROW DATASET ====================

## create dataset with an independent variable
large_dataset <- data.frame(
	x= runif(min= 1, max= 100, n= 10^7),
	y= NA
	)

## create a dependent variable with a polynomial relationship to I.V.
f <- function(x){ {-6 * x^5} + {3 * x^4} + {-5 * x^3} + {3 * x^2} + {-5 * x} }
large_dataset$y <- f(large_dataset$x)
remove(f)


## YOU COULD CREATE A TESTING MODE LIKE THIS ====================

## settings
my_settings <- list(
	test_mode= TRUE
	)

## if in test mode, take a sample of the dataset
if (my_settings$test_mode) {
	
	## Reduce size of dataset by 80%
	sample_index <- floor(length(large_dataset$y) * 0.2)
	set.seed(464)
	sample_index <- sample(
		x= seq(along= large_dataset$y),
		size= sample_index
		)
	large_dataset <- large_dataset[sample_index, ]
	
	## Throw a warning that script is in test mode
	warning("Script is currently running in test mode!")
	
  } else {
	
  	## print start time for script that may run for a long time
  	print(paste("Script initiated at:", Sys.time()))
	  print("Script is running in standard mode")
	
  }

## conduct some analysis of dataset
Sigmoid <- function(x){
	y <- exp(x) + 1
	y[y == 0] <- NA
	y <- exp(x) / y
	return(y)
	}
sigmoid_correlation <- cor( Sigmoid(large_dataset$x), Sigmoid(large_dataset$y) )

print(
	paste(
		"The analysis indicates a sigmoid correlation of:",
		round(sigmoid_correlation, 3)
		)
	)

## end code with restatement of test mode warning
if (my_settings$test_mode) warning("Script is currently running in test mode!")

## remove demonstration objects
remove(sigmoid_correlation, large_dataset, my_settings, Sigmoid)

```

Another common use for conditional expressions is making functions able to adapt to different kinds of inputs.  For example, `sample()` is able to receive either a `x=` and `size=` argument or just a `x=` argument. Below is an example function that uses conditional expression to accomplish a similar result.  The example uses `&&` and `||`.  These operators are forms of the standard logical operators that ignore all but the first element of the logical vector.  They are considered "safer" than the standard operators is circumstances where there should only be one element in the logical vectors evaluated.  The example also uses `stop()`, which is the function for generating an error message.

```{R Function Model Conditionals}
TwoModeSample <- function(x, size= NULL) {
	
	## interpret special case #1
	if (length(x) == 1 && is.null(size) && is.numeric(x)) {
		if (x < 1)  return(x) else x <- 1:x
	}
	
	## shuffle x (this is one way of taking a sample)
	sample_index <- runif(n= length(x))
	sample_index <- order(sample_index)
	x <- x[sample_index]
	
	## interpret special case #2
	if (is.null(size)) return(x) else {
		
		## error check the size input
		size <- as.integer(size)
		if (size < 1 || size > length(x)) stop("Invalid \'size\' argument")
		
		## return sample
		return(x[1:size[1]])
		
		}
	
}


## test function with x= and size= arguments
TwoModeSample(month.abb, 3)

## test function with x= a vector and no size= argument
TwoModeSample(month.abb)

## test function with x= a single integer and no size= argument
TwoModeSample(5)
TwoModeSample(-5)

## test function with an improperly long size argument (the || prevents this
  ## from causing problems)
TwoModeSample(month.abb, 3:4)

## these tests should cause errors and do
# TwoModeSample(month.abb, -3)
# TwoModeSample(month.abb, 15)
```

In the example above, take note of two things:

1. `return()` is used to end the function and express the result whenever the result has been ascertained for a given test case.  When used like this, `return()` causes the function to skip the lines of code that follow it.

2. As with `break` and `repeat` in the loops section, `else` does not have to follow the standard `{}` conventions of R syntax.  In general, it is better to brace than to not brace but either is considered acceptable.

### Error Handling

#### Tools for Solving Specific Error Problems

When programming, errors are not the enemy.  Error and warning messages let you know that something has gone wrong in the code, providing you with an opportunity to resolve the problem.  As a programmer, you should be more worried about things going wrong _without_ errors and warnings.  Here are some tools to consider as you work to manage errors and warnings.

`stop()` is the R function for generating error messages. Use `stop()` to force your code to abort when something happens that should not happen.  One way to do this is to write a `TRUE`/`FALSE` test, pass the test result as an argument to if `if(){}`, and then call `stop()` inside the braces.  While `stop()` will abort the execution of a particular function/line of code, some R GUI will continue to run the lines after an aborted line and others will not.  `try()` prevents an error from causing a function / line to abort.  Instead of generating an error, an errant function encased in `try()`, using the `silent= TRUE` argument, will return a `try-error` class object, as demonstrated below.

```{R try()}

## express a non-existent object to generate an error
try_output <- try(object_that_does_not_exist, silent= TRUE)

## express the try-error object that results
class(try_output)
try_output

## build a hook to react to try-error -- creating missing object with
  ## default values
if (any(class(try_output) %in% "try-error")) {
	object_that_does_not_exist <- rep(0, 10)
}

## express newly created object
object_that_does_not_exist

## delete demonstration objects
remove(try_output, object_that_does_not_exist)

```

`warning()` is the R function for generating warning messages.  You can use it in the same way as `stop()`.  However, a warning message will not prevent a function / line of code from executing.  `warnings()` will enable you to view all of the warning messages thrown during the execution of a script.  `suppressWarnings()` will pass on the expression it encases, while silencing the error messages.  Below is an example of `supressWarnings()` in action

```{R suppressWarnings()}
## generates a warning
sqrt(-1)

## silences the warning
suppressWarnings(sqrt(-1))
```

`unclass()` strips the customized class attributes from an object.  This will cause the object to display in a more standard object format.  `unclass()` does not change the object itself, just the way R interprets it.  In general, you will find that most package-specific R objects are actually just lists with fancy display methods.

```{R unclass()}
## this is a t-test object with a t-test class attribute
t.test(1:10, 11:20)

## this is a t-test object with the attribute stripped, revealing the basic
  ## list object structure underneath
unclass(t.test(1:10, 11:20))
```

`gc()` triggers RAM clean-up.  When you perform operations involving large amounts of data, that data occupies large blocks of RAM.  When you delete large data objects from memory, that memory space is eventually returns to the pool of available memory.  This process is called garbage collection.  Like actual garbage collection, deciding when to do memory garbage collection requires efficiency trade-offs.  Do it too often and you waste time and system resources that you could better use elsewhere.  Do it too infrequently and the metaphorical garbage piles up, taking up space that could be better used to store useful things.  R automatically makes decisions about when to do garbage collection.  However, there are circumstances where you can avoid errors if you manually trigger collection.  For example, if you load a data object that occupies most of your RAM, your first task will likely to be filter, summarize, or transform that data into the smallest possible format that you will need for your analysis.  Your second might be to use `gc()`, triggering garbage collection to free the RAM that the raw data object previously occupied.

The `::` operator enables you to call a function from a package without first loading the package using `library` or in circumstances where the function is masked.  This can be particularly useful for working with parallelized operations, where the threads may have difficulty accessing package dependencies.  Below is an example of `::` in action, used to resolve issues with function masking.

```{R Double Colon}

## load the parallel processing package
library(parallel)

## use the parallel package's function for detecting the number of cores on your
  ## machine
detectCores()

## create a new function with the same name (this is a bad practice but can
  ## happen when you load packages from different creators.  Since the global
  ## environment has precedence over package environments, this masks the
  ## version of detectCores() in the parallel package
detectCores <- function(){"Cores? What cores?"}
detectCores()

## using :: you can use it anyway
parallel::detectCores()

## delete demonstration objects and reset the environment
remove(detectCores)
detach(package:parallel)

```

#### Tools for Solving Specific Console Output Problems

Keep the amount of expressions (as opposed to assignments) in your code to a minimum.  If a line of code does not change any object, it is likely cluttering your script without providing benefit.  However, there are circumstances where it is important to enable your script to communicate with you (or anyone else who is running your code). R offers a variety of tools for generating messages or interacting with the expressed messages of other programmers.

`identity()`, `print()`, and `cat()` each enable you to express arguments to console.  `print()` and `cat()` express their arguments to the console, regardless of whether they are buried in layers of code like functions and loops.  `identity()` expresses its arguments using standard expression.  It is equivalent to defining the function `function(x){x}`.  In contrast to `cat()` and `print()`, it will only express its argument within the environment (or loop) that contains it.  

While `cat()` and `print` are similar, `cat()` works differently than `print()` in at least three ways.  First, it will combine multiple expressions into one output.  Second, it bypasses object expression and directly prints messages to the console.  Third, since it bypasses expression, you cannot assign its output to an object using just the `<-` operator.

Below is a demonstration of the three functions in action.

```{R print() and cat()}

## use print()/cat() to express output directly to console
identity("I know who I am!")
print("I am a machine that puts ink on paper!")
cat("I am small, furry carnivore.", "Meow!", "Purrrrrr!")

## define a function to test expression option inside a loop inside a function
DemonstrationFunction <- function(expr, FUN) {
	for (iter in 1:10) {
		if (iter == 7) { FUN(expr) }
		}
}

## test whether each function can express through multiple code layers
the_expression <- "This is a demonstration"
DemonstrationFunction(the_expression, identity)
DemonstrationFunction(the_expression, print)
DemonstrationFunction(the_expression, cat)

## test how each function responds to assignment
self_identification <- identity("I know who I am!")
self_identification

printer <- print("I am a machine that puts ink on paper!")
printer

kitty_cat <- cat("I am small, furry carnivore.", "Meow!", "Purrrrrr!")
kitty_cat
```

You may wish to get rid of the communications that have been built into some of the functions you use.  Alternately, you may wish to capture the non-expression console outputs for other functions you use. `invisible()` prevents functions from showing up in the terminal.  As discussed earlier, `supressWarnings()` performs a similar service for warnings messages.  Conversely, `capture.output()` will capture the messages that a function outputs to console, even when the function does not express those messages as assignable output.

```{R invisible() and capture.output()}

##  invisible() suppresses console text; leaves assignment operations untouched
invisible("I am very happy!")
very_expressive <- invisible("I am very happy!")
very_expressive

## capture.output() diverts console output to the assignment destination object,
  ## even when assignment would typically come up empty
kitty_cat <- capture.output(
	cat("I am small, furry carnivore.", "Meow!", "Purrrrrr!"))
kitty_cat

## used in tandem invisible() and capture.output() can completely muzzle print()
printer <- "I am a machine that puts ink on paper!"
invisible(print(printer))
capture.output(print(printer))
invisible(capture.output(print(printer)))

## delete demonstration objects
remove(DemonstrationFunction, kitty_cat, printer, sample_index,
	self_identification, the_expression, TwoModeSample, very_expressive)
```

### Analyzing Objects and Functions


#### Analyzing Objects

Lists are the favored object structure for the specialized objects underlying most packages because a list can hold an arbitrary number of objects within it. If you `unclass()` most specialty objects in R, you will find a list underneath.  However, since lists nest inside other lists, complex list objects can be difficult to understand at a glance.  While it takes some practice before `str()` outputs become intuitive, this function can be powerful tool for making sense of complex lists.

```{R str()}

## generate a list with complex sub-lists
x <- 1:10
y <- x^2
complex_list <- list(
	"A"= list(
		"String"= paste(letters, LETTERS),
		"Integers"= 100:110
	  ),
	"B"= list(
		"T Test List"= unclass(t.test(x, y)),
		"Factors"= as.factor(month.abb)
			)
)

## use str() to analyse the object, starting with the top level
str(complex_list, max.level= 1)

## adjust max.level to dive deeper into the list - 2 levels
str(complex_list, max.level= 2)

## adjust max.level to dive deeper into the list - 3 levels
str(complex_list, max.level= 3)

## remove demonstration objects
remove(complex_list, x, y)

```

Meta-data is information about a body of data.  In R, there are multiple options for attaching meta-data to an object.  We have already encountered meta-data in the form of `names()`, `dimnames()`, and `class()`.  Each of these are "attributes" of objects.  The `attributes()` function will express all of the attributes currently attached to an object.  `attr()` can retrieve a specific attribute by name or enable you to add or edit attributes.

```{R Attributes}

## make a dataset
a_dataset <- data.frame(
	x= round(seq(from= 1/24, to= 23/24, length.out= 12) * 100),
	y= month.name,
	z= as.logical({1:12} %% 2)
	)
head(a_dataset)

## explore its existing attributes
attributes(a_dataset)

attr(a_dataset, "row.names")
rownames(a_dataset) # this function does the same attribute pull

attr(a_dataset, "class")
class(a_dataset) # this function does the same attribute pull

## change the row.names attribute to month.abb
head(a_dataset)
attr(a_dataset, "row.names") <- month.abb
head(a_dataset)

## add a new attribute
attr(a_dataset, "short.months") <- "Feb"
attributes(a_dataset)

## delete demonstration object
remove(a_dataset)

```

#### Analyzing functions

As discussed in the Conditionals section, many R functions are adaptive.  Depending on the characteristics of the objects supplied to its arguments, the function will do different things. R accomplishes this by attaching "methods" to a "generic" function, where each method is a separate function.  Methods are hidden a layer deeper in the code to prevent confusion and make the user experience easier.  However, there are times when analyzing a method in detail will enable you to resolve errors, improve efficiency and better document procedures. 

Below is a demonstration of how to retrieve a method.  `methods()` will express the available methods for a generic function.  `getAnywhere()` will enable you to retrieve them.

```{R Method Retrieval}

## the summary() function does different things, depending on the class of the
  ## object
a_dataframe <- data.frame(x= as.factor(month.abb), y= 1:12, z= month.name)
a_list <- list(x= as.factor(month.abb), y= 1:12, z= month.name)
a_factor <- as.factor(month.abb)

## express the results (as a reminder, expressions should be written directly
  ## into your console, not saved to script
summary(a_dataframe)
summary(a_list)
summary(a_factor)

## express methods summary() can access
methods(summary)

## retrieve a specific method and make sure it works
  ## for summary.factor(), the getAnywhere() is not necessary.  However, it
  ## will be necessary in some cases, so I will demonstrate it here.
RetrievedMethod <- getAnywhere(summary.factor)$obj[[1]]
RetrievedMethod(a_factor)

## delete demonstration objects
remove(a_dataframe, a_list, a_factor, RetrievedMethod)

```

Once you have retrieved a method, you can apply various techniques to understand how it works and what its limitations might be. `Rprof()` and `summaryRprof()` can make clear what components are drawing most heavily on computational resources.  If the function is well-written, these should be the parts of the function that critical tasks.  Doing a line by line dissection can you help gain further insight.

```{R Function Dissection}
## make a test object
test_factor <- sample(state.abb, size= 10^8, replace= TRUE)
test_factor <- factor(test_factor, levels= state.abb)

## profile function to understand what components draw the most resources
Rprof()
summary.factor(test_factor)
Rprof(NULL)
profile_results <- summaryRprof()
file.remove("Rprof.out")

## express profile results - indicates that function ran in roughly 2.5 seconds
  ## and that table() accounts nearly 90 percent of the run time
profile_results

## delete demonstraiton objects
remove(profile_results)

```

Once you have profiled the function, you can do a line by line dissection.  Here is a process you might follow.

Step one: copy/paste the function into a script and remove the function packaging.

```{R Function Dissection, Step 1}

## pull function into R (express it, then copy/paste and do line-by-line analysis

object <- test_factor[1:10^3] # make erstwhile arguments freestanding objects
  # so that you can test each line to see how it is changing the argument objects
maxsum <- 10^2

## function (object, maxsum = 100L, ...) { # comment out function packaging
    nas <- is.na(object)
    ll <- levels(object)
    if (ana <- any(nas)) maxsum <- maxsum - 1L
    tbl <- table(object)
    tt <- c(tbl)
    names(tt) <- dimnames(tbl)[[1L]]
    if (length(ll) > maxsum) {
        drop <- maxsum:length(ll)
        o <- sort.list(tt, decreasing = TRUE)
        tt <- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]]))
    }
    if (ana) c(tt, `NA's` = sum(nas)) else tt
## } # comment out function packaging

```

Step two: determine what each line does.  Sometimes, you will be able to do this just by looking at the code.  In other cases, you will need to run the lines and inspect how the argument objects are changing from line to line.  This is why you make the arguments into freestanding objects.

```{R Function Dissection, Step 2}

## pull function into R (express it, then copy/paste and do line-by-line analysis

object <- test_factor[1:10^3] # make erstwhile arguments freestanding objects
maxsum <- 10^2 # the documentation says this limits how many factors levels to
  # show for the table counting occurance of factors... this prevents console
  ## flood if there are more than 100 unique factors in the object

## function (object, maxsum = 100L, ...) { # comment out function packaging
    nas <- is.na(object) # detect NA (logical vector)
    ll <- levels(object) # find the levels of the factor object
    if (ana <- any(nas)) maxsum <- maxsum - 1L # special handling instructions
      # if NA found, drop the last element to be printed this makes room for an
      # NA count to be tacked on at the bottom
    tbl <- table(object) # count how often each factor appears in object
    tt <- c(tbl) # converts table to integer, while preserving the element names
    names(tt) <- dimnames(tbl)[[1L]] # safety code to make sure names aren't lost
    if (length(ll) > maxsum) { ## console flood prevention (see maxsum note)
    	## will also drop NA element from table under a specific edge case
        drop <- maxsum:length(ll) ## index of which elements to drop
        o <- sort.list(tt, decreasing = TRUE) ## makes sure least common factors
          # get dropped from the occurrence count table
        tt <- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]])) # drop least common factors
          ## from table, but then sum dropped counts into an "(other)" category
    }
    if (ana) c(tt, `NA's` = sum(nas)) else tt # tack on NA's if any were found
        
## } # comment out function packaging

```

Step three: summary what each meaningful chunk of code does. Rearrange lines if needed.

```{R Function Dissection, Step 3}

## pull function into R (express it, then copy/paste and do line-by-line analysis

object <- test_factor[1:10^3] # make erstwhile arguments freestanding objects
maxsum <- 10^2 # the documentation says this limits how many factors levels to
  # show for the table counting occurance of factors... this prevents console
  ## flood if there are more than 100 unique factors in the object

## function (object, maxsum = 100L, ...) { # comment out function packaging




  ## DETECT NAs; LAY GROUND WORK FOR NA HANDING AT END OF FUNCTION

    nas <- is.na(object) # detect NA (logical vector)
    if (ana <- any(nas)) maxsum <- maxsum - 1L # special handling instructions
      # if NA found, drop the last element to be printed this makes room for an
      # NA count to be tacked on at the bottom
    
  ## COUNT HOW OFTEN EACH UNIQUE FACTOR APPEARS
    
    tbl <- table(object) # count how often each factor appears in object
    tt <- c(tbl) # converts table to integer, while preserving the element names
    names(tt) <- dimnames(tbl)[[1L]] # safety code to make sure names aren't lost
    
  ## DROP RARE FACTOR COUNTS IF TABLE IS TOO BIG, SUM DROPPED FACTOR COUNTS AS
    ## AN "OTHER" CATEGORY
    
     ll <- levels(object) # find the levels of the factor object
      ## (moved from original spot as line #2   
    if (length(ll) > maxsum) { ## console flood prevention (see maxsum note)
    	## will also drop NA element from table under a specific edge case
        drop <- maxsum:length(ll) ## index of which elements to drop
        o <- sort.list(tt, decreasing = TRUE) ## makes sure least common factors
          # get dropped from the occurrence count table
        tt <- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]])) # drop least common factors
          ## from table, but then sum dropped counts into an "(other)" category
    }
     
     
  ## TACK ON AN NA CATGORY IF NA WERE FOUND
     
    if (ana) c(tt, `NA's` = sum(nas)) else tt # tack on NA's if any were found
       ## this set-up (NA detection code at the front of a function, NA handling
       ## code at the back) is common among R functions.
        
## } # comment out function packaging
     

## delete demonstration objects
remove(ana, ll, maxsum, nas, tbl, test_factor, tt, object)

```

When you analyze a function like this, you may discover assumptions built into the code that differ from what you thought or what your project requires.  You can then better drop these assumptions or devise remedies as needed.  Even if the function does exactly what you expect, there still may be opportunities to develop more efficient code.  In this case, a natural performance question is whether an alternative to the existing code may perform better.  We can test this using the time trial approach.

From `Rprof()` we know that `table()` is driving run time.  For our line-by-line analysis, we know that this is reasonable because the summary consists of a frequency count of factors and `table()` provides that count.  The rest of the code consists of special modifications to handle unusual situations ("edge cases") where the number of unique factors is really big or the factor vector contains `NA`s.  Perhaps an alternative to `table()`, such as `tapply(x, x, length)` would run quicker? Let's find out!

```{R Time Trial - table() vs tapply()}

## create modified version of the function
FactorSummary <- function (object, maxsum = 100L) {
	
	  ## detect NAs
    nas <- is.na(object)
    if (ana <- any(nas)) maxsum <- maxsum - 1L
    
    ## Count occurrences of each unique factor
    tbl <- tapply(object, object, length) #swap tapply() in for table()
    tt <- c(tbl)
    names(tt) <- dimnames(tbl)[[1L]]
    
    ## Sum least common factors as "Other" if too many unique factors
    ll <- levels(object)
    if (length(ll) > maxsum) {
        drop <- maxsum:length(ll)
        o <- sort.list(tt, decreasing = TRUE)
        tt <- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]]))
    }
    
    ## Count NAs if applicable
    if (ana) c(tt, `NA's` = sum(nas)) else tt
}

## Create object to store the results of the time trials
trial_results <- array(NA, dim= c(2^3, 2))
colnames(trial_results) <- c("table", "tapply")

## generate large test object
test_factor <- sample(state.abb, size= 10^8, replace= TRUE)
test_factor <- factor(test_factor, levels= state.abb)
test_factor[2^{2:10}] <- NA

# pre-initialize start time variable so all trials have level playing field 
trial_time <- Sys.time() 

for (iter in seq(nrow(trial_results))) {
	
	## time summary.factor()
	trial_time <- Sys.time()
	invisible(summary.factor(test_factor, maxsum= 10))
	trial_results[iter, "table"] <- Sys.time() - trial_time
	
	## time FactorSummary()
	trial_time <- Sys.time()
	invisible(FactorSummary(test_factor, maxsum= 10))
	trial_results[iter, "tapply"] <- Sys.time() - trial_time	
	
	}

## calculate results
trial_results <- colMeans(trial_results)
trial_results <- trial_results / min(trial_results)

## conclusion: table() yields significantly lower run times than tapply() for
  ## summary.factor()
round(trial_results, 2)

```

==========

### Vocabulary Table for Lesson B5

In order to program effectively, you will need to memorize basic functions, operators, and constants.  Write each of the functions/operators/constants below on a flash card.  On the back of each card, write a succinct definition of what it does and a example of a line code you could enter into console that uses it.  Drill with these cards until you have memorized them.  Then drill again, coming up with a fresh example for each and testing that example in the console.

In order to understand what each function/operator/constant does, use the `help()` function to pull the documentation for it.  For example, `help("objects")` would pull up the documentation for the function `objects()`.  This document includes a description of what the function does ("Description" section), a list of all the arguments that can be given to the function ("Arguments" section), and examples of how to use the function ("Examples" section) at the bottom.  Only copy the definition or example from the documentation to your flash card if you absolutely understand what it does.  Otherwise, substitute your own.

The help documentation may be a difficult to read at first but keep practicing.  Over time, getting useful information from the documentation will become effortless.  Resist the impulse to do a Google search before you have consulted the documentation.  Google results can be of mixed quality - sometimes you will get a thoughtful, efficient solution, sometimes you will get a byzantine work-around that will teach you bad habits.

|Loops   |Conditions	|Error Handling|Console Expression	|Analysis Tools
|:-					|:-				|:-						|:-										|:-							
|`repeat{}`	|`if(){}`	|`gc()`				|`cat()`							|`attr()`				
|`while(){}`|`else{}`	|`::`					|`capture.output()`		|`attributes()`	
|`for(){}`	|`&&`			|`stop()`			|`invisible()`				|`str()`				
|`break`		|`||`			|`unclass()`	|`print()`						|`Rprof()`					
|`next`			|					|`try()`			|`suppressWarnings()`	|`summaryRprof()`		
|`optim()`	|					|`warnings()`	|`identity()`					|`methods()`
|`Sys.sleep()`|				|`warning()`	|											|`getAnywhere()`